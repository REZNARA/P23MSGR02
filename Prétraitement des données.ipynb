{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12d1b298",
   "metadata": {},
   "source": [
    "# Prétraitement des données\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9622db15",
   "metadata": {},
   "source": [
    "### Chargement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e979314",
   "metadata": {},
   "source": [
    "Le fichier Data_Tweets.parquet contient que deux colonnes id et tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6777014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    " \n",
    "df1 = pd.read_parquet(\"./Data/Data_Tweets.parquet\", columns=['id', 'text'])  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb78290a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum index: 14634241\n"
     ]
    }
   ],
   "source": [
    "max_index = df1.index.max()  \n",
    "print('Maximum index:', max_index)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eede697f",
   "metadata": {},
   "source": [
    "Dans ce code, nous avons utilisé la bibliothèque Natural Language Toolkit (NLTK) de Python pour prétraiter les tweets d'une dataframe. Nous avons téléchargé les ressources NLTK nécessaires, y compris les mots vides en anglais, le WordNetLemmatizer et les outils de tokenisation. Nous avons ensuite défini une fonction pour prétraiter chaque tweet, qui supprime les caractères spéciaux, supprime les mots vides, lemmatize les mots restants et les joint pour former une chaîne unique. Enfin, nous avons appliqué cette fonction à chaque tweet dans la colonne 'text' de la dataframe et stocké les résultats dans une nouvelle colonne 'preprocessed_tweet'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae343ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ahmidich\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ahmidich\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ahmidich\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk      \n",
    "import re    \n",
    "from nltk.corpus import stopwords      \n",
    "from nltk.tokenize import word_tokenize      \n",
    "from nltk.stem import WordNetLemmatizer      \n",
    "    \n",
    "# Télécharger les ressources NLTK nécessaires      \n",
    "nltk.download('punkt')      \n",
    "nltk.download('stopwords')      \n",
    "nltk.download('wordnet')      \n",
    "nltk.download('omw-1.4')  \n",
    "  \n",
    "# Définir les mots vides à supprimer      \n",
    "stop_words = set(stopwords.words('english'))      \n",
    "    \n",
    "# Définir le lemmatizer      \n",
    "lemmatizer = WordNetLemmatizer()      \n",
    "    \n",
    "# Définir une fonction pour prétraiter un seul tweet      \n",
    "def preprocess_tweet(tweet):      \n",
    "    # Supprimer les caractères spéciaux du texte du tweet    \n",
    "    tweet = re.sub(r'[^\\w\\s]', '', tweet)      \n",
    "        \n",
    "    # Tokeniser le texte du tweet en mots      \n",
    "    words = word_tokenize(tweet)      \n",
    "    \n",
    "    # Supprimer les mots vides des mots tokenisés      \n",
    "    words = [word for word in words if word.lower() not in stop_words]      \n",
    "    \n",
    "    # Lemmatizer les mots restants      \n",
    "    words = [lemmatizer.lemmatize(word) for word in words]      \n",
    "    \n",
    "    # Joindre les mots prétraités pour former une chaîne unique      \n",
    "    preprocessed_tweet = ' '.join(words)      \n",
    "    \n",
    "    return preprocessed_tweet      \n",
    "    \n",
    "  \n",
    "# Appliquer la fonction preprocess_tweet à chaque tweet dans la colonne 'text' de la dataframe df1  \n",
    "\n",
    "df1['preprocessed_tweet'] = df1['text'].apply(preprocess_tweet)  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ad53846",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv('preprocesses_tweets.csv', index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
